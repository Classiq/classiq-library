{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Hybrid Classical-Quantum Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Classical Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "Neural networks is one of the major branches in machine learning, with wide use in applications and research. A neural network—or, more generally, a deep neural network—is a parametric function of a specific structure (inspired by neural networks in biology), which is trained to capture specific functionality.\n",
    "\n",
    "In its most basic form, a neural network for learning a function $\\vec{f}: \\mathbb{R}^N\\rightarrow \\mathbb{R}^M$ looks as follows:\n",
    "1. There is an input vector of size $N$ (red circles in Fig. 1).\n",
    "2. Each entry of the input goes into a hidden layer of size $K$, where each neuron (blue circles in Fig. 1) is defined with an \"activation function\" $y^{k}(\\vec{w}^{(1)}; \\vec{x})$ for $k=1,\\dots,K$, and $\\vec{w}^{(1)}$ are parameters.\n",
    "3. The output of the hidden layer is sent to the output layer (green circles in Fig. 1) $\\tilde{f}^{m}(\\vec{w}^{(2)};\\vec{y})$ for $m=1,\\dots,M$, and $\\vec{w}^{(2)}$ are parameters.\n",
    "\n",
    "The output $\\vec{\\tilde{f}}$ is thus a parametric function (in $\\vec{w}^{(1)},\\,\\vec{w}^{(2)}$), which can be trained to capture the target function $\\vec{f}$.\n",
    "\n",
    "\n",
    "![png](neural_network.png)\n",
    "\n",
    "<center>\n",
    "<figcaption align = \"middle\"> Figure 1. A single layer classical neural network (from Wikipedia). Here, the input size is $N=3$, the output size is $M=3$, and the hidden layer has $L=4$ neurons. </figcaption>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "**Deep neural networks** are similar to the description above, having more than one hidden layer. This provides a more complex structure that can capture more complex functionalities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Quantum Neural Networks\n",
    "\n",
    "The idea of a quantum neural network refers to combining parametric circuits as a replacement for all or some of the classical layers in classical neural networks. The basic object in QNN is thus a **quantum layer**, which has a classical input and returns a classical output. The output is obtained by running a quantum program. A quantum layer is thus composed of three parts:\n",
    "1. A quantum part that encodes the input: This is a parametric quantum function for representing the entries of a single data point. There are three canonical ways to encode a data vector of size $N$: [angle-encoding](https://github.com/Classiq/classiq-library/blob/main/functions/qmod_library_reference/classiq_open_library/variational_data_encoding/variational_data_encoding.ipynb) using $N$ qubits, [dense angle-encoding](https://github.com/Classiq/classiq-library/blob/main/functions/qmod_library_reference/classiq_open_library/variational_data_encoding/variational_data_encoding.ipynb) using $\\lceil N/2\\rceil$ qubits, and amplitude-encoding using $\\lceil\\log_2N\\rceil$ qubits.\n",
    "2. A quantum ansatz part: This is a parametric quantum function, whose parameters are trained as the weights in classical layers.\n",
    "3. A postprocess classical part, for returning an output classical vector.\n",
    "\n",
    "The integration of quantum layers in classical neural networks may offer reduction in resources for a given functionality, as the network (or part of it) is expressed via the Hilbert space, providing different expressibility compared to classical networks.\n",
    "\n",
    "This notebook demonstrates QNN by treating a specific function—the subset majority—for which we construct, train, and verify a hybrid classical-quantum neural network. The notebook assumes familiarity with Classiq and NN with PyTorch. See the [QML guide with Classiq](https://github.com/Classiq/classiq-library/blob/main/tutorials/basic_tutorials/qml_with_classiq_guide/qml_with_classiq_guide.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Example: Hybrid Neural Network for the Subset Majority Function\n",
    "\n",
    "\n",
    "For an integer $N$ and a given subset of indices $S \\subset \\{0,1,\\dots,N\\}$ we define the subset majority function, $M_{S}:\\{0,1\\}^{\\times N}\\rightarrow \\{0,1\\}$ that acts on binary strings of size $N$ as follows: it returns 1 if the number of ones within the substring according to $S$ is larger than $|S|//2$, and 0 otherwise,\n",
    "$$\n",
    "M_S(\\vec{b}) = \\left\\{ \\begin{array}{l l }\n",
    "1 & \\text{if } \\sum_{j\\in S} b_{j}>|S|//2, \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{array}\n",
    "\\right .\n",
    "$$\n",
    "\n",
    "For example, we consider $N=7$ and $S=\\{0,1,4\\}$:\n",
    "* The string 0101110 corresponds to the substring 011, for which the number of ones is 2(>1). Therefore, $M_S(0101110)=1$.\n",
    "* The string 0011111 corresponds to the substring 001, for which the number of ones is 1(=1). Therefore, $M_S(0101110)=0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Generating Data for a Specific Example\n",
    "\n",
    "Let us consider a specific example for our demonstration. We choose $N=10$ and generate all possible data of $2^N$ bit strings. We also take a specific subset $S=\\{1, 3, 4, 6, 7, 9\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq -U \"classiq[qml]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(1)\n",
    "\n",
    "STRING_LEN = 10\n",
    "majority_data = [\n",
    "    [int(d) for d in np.binary_repr(k, STRING_LEN)] for k in range(2**STRING_LEN)\n",
    "]\n",
    "random.shuffle(majority_data)  # shuffling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBSET_INDICES = [1, 3, 4, 6, 7, 9]\n",
    "subset_indicator = np.zeros(STRING_LEN)\n",
    "subset_indicator[SUBSET_INDICES] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "majority = (majority_data @ subset_indicator > len(SUBSET_INDICES) // 2) * 1\n",
    "labels = [[l] for l in majority]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "We choose data for training and data for verification, and define the batch size for the corresponding data loaders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_SIZE = 340\n",
    "TEST_SIZE = 512\n",
    "\n",
    "training_data = majority_data[0:TRAINING_SIZE]\n",
    "training_labels = labels[0:TRAINING_SIZE]\n",
    "test_data = majority_data[TRAINING_SIZE : TRAINING_SIZE + TEST_SIZE]\n",
    "test_labels = labels[TRAINING_SIZE : TRAINING_SIZE + TEST_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "training_dataset = TensorDataset(\n",
    "    torch.Tensor(training_data), torch.Tensor(training_labels)\n",
    ")  # create dataset\n",
    "training_dataloader = DataLoader(\n",
    "    training_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=False\n",
    ")  # create dataloader\n",
    "\n",
    "test_dataset = TensorDataset(\n",
    "    torch.Tensor(test_data), torch.Tensor(test_labels)\n",
    ")  # create your dataset\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=False\n",
    ")  # create your dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Constructing a Hybrid Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "We build the following hybrid neural network:\n",
    "\n",
    "**Data flattening $\\rightarrow$ A classical linear layer of size 10 to 4 with `Tanh` activation $\\rightarrow$ A qlayer of size 4 to 2 $\\rightarrow$ a classical linear layer of size 2 to 1 with `ReLU` activation.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "The classical layers can be defined with PyTorch built-in functions. The quantum layer is constructed with \n",
    "\n",
    "(1) a [dense angle-encoding](https://github.com/Classiq/classiq-library/blob/main/functions/qmod_library_reference/classiq_open_library/variational_data_encoding/variational_data_encoding.ipynb) function \n",
    "\n",
    "(2) a simple ansatz with RY and RZZ rotations \n",
    "\n",
    "(3) a postprocess that is based on a measurement per qubit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "#### The Quantum Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from classiq import *\n",
    "from classiq.applications.qnn.types import SavedResult\n",
    "from classiq.execution import ExecutionPreferences, execute_qnn\n",
    "\n",
    "\n",
    "@qfunc\n",
    "def my_ansatz(weights: CArray[CReal], qbv: QArray) -> None:\n",
    "    \"\"\"\n",
    "    Gets a quantum variable of $m$ qubits, and applies RY gate on each qubit and RZZ gate on each pair of qubits\n",
    "    in a linear connectivity. The classical array weights represents the $2m-1$ parametric rotations.\n",
    "    \"\"\"\n",
    "    repeat(\n",
    "        count=qbv.len,\n",
    "        iteration=lambda index: RY(weights[index], qbv[index]),\n",
    "    )\n",
    "    repeat(\n",
    "        count=qbv.len - 1,\n",
    "        iteration=lambda index: RZZ(weights[qbv.len + index], qbv[index : index + 2]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum program link: https://platform.classiq.io/circuit/32pQqOSulTv1Qvssd71mPgf4btU\n"
     ]
    }
   ],
   "source": [
    "QLAYER_SIZE = 4\n",
    "num_qubits = int(np.ceil(QLAYER_SIZE / 2))\n",
    "num_weights = 2 * num_qubits - 1\n",
    "NUM_SHOTS = 4096\n",
    "\n",
    "\n",
    "@qfunc\n",
    "def main(\n",
    "    input: CArray[CReal, QLAYER_SIZE],\n",
    "    weight: CArray[CReal, num_weights],\n",
    "    result: Output[QArray],\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    The quantum part of the quantum layer.\n",
    "    The prefix for the data loading parameters must be set to `input_` or `i_`.\n",
    "    The prefix for the ansatz parameters must be set to `weights_` or `weight`\n",
    "    \"\"\"\n",
    "    encode_on_bloch(input, result)\n",
    "    my_ansatz(weights=weight, qbv=result)\n",
    "\n",
    "\n",
    "qmod = create_model(\n",
    "    main, execution_preferences=ExecutionPreferences(num_shots=NUM_SHOTS)\n",
    ")\n",
    "qprog = synthesize(qmod)\n",
    "show(qprog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_post_process(result: SavedResult, num_qubits, num_shots) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Classical postprocess function.\n",
    "    Gets the histogram after execution and returns a vector $\\vec{y}$,\n",
    "    where $y_i$ is the probability of measuring 1 on the $i$-th qubit.\n",
    "    \"\"\"\n",
    "    res = result.value\n",
    "    yvec = [\n",
    "        (res.counts_of_qubits(k)[\"1\"] if \"1\" in res.counts_of_qubits(k) else 0)\n",
    "        / num_shots\n",
    "        for k in range(num_qubits)\n",
    "    ]\n",
    "\n",
    "    return torch.tensor(yvec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "#### The Full Hybrid Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "Now, we can define the full network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "from classiq.applications.qnn import QLayer\n",
    "\n",
    "\n",
    "def create_net(*args, **kwargs) -> nn.Module:\n",
    "    class Net(nn.Module):\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            super().__init__()\n",
    "            self.flatten = nn.Flatten()\n",
    "            self.linear_1 = nn.Linear(STRING_LEN, 4)\n",
    "            self.activation_1 = nn.Tanh()\n",
    "            self.linear_2 = nn.Linear(2, 1)\n",
    "            self.activation_2 = nn.ReLU()\n",
    "\n",
    "            self.qlayer = QLayer(\n",
    "                qprog,\n",
    "                execute_qnn,\n",
    "                post_process=lambda res: my_post_process(\n",
    "                    res, num_qubits=num_qubits, num_shots=NUM_SHOTS\n",
    "                ),\n",
    "                *args,\n",
    "                **kwargs,\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.flatten(x)\n",
    "            x = self.linear_1(x)\n",
    "            x = self.activation_1(x)\n",
    "            x = self.qlayer(x)  # 4 to 2\n",
    "            x = self.linear_2(x)  # 2 to 1\n",
    "            x = self.activation_2(x)\n",
    "            return x\n",
    "\n",
    "    return Net(*args, **kwargs)\n",
    "\n",
    "\n",
    "my_network = create_net()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Training and Verifying the Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "We define some hyperparameters such as loss function and optimization method, and a training function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "LEARNING_RATE = 0.05\n",
    "\n",
    "# choosing our loss function\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "# choosing our optimizer\n",
    "optimizer = optim.SGD(my_network.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "Next, we define a `train` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as time\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def train(\n",
    "    network: nn.Module,\n",
    "    data_loader: DataLoader,\n",
    "    loss_func: nn.modules.loss._Loss,\n",
    "    optimizer: optim.Optimizer,\n",
    "    epoch: int = 20,\n",
    ") -> None:\n",
    "    for index in range(epoch):\n",
    "        start = time.time()\n",
    "        for data, label in data_loader:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = network(data)\n",
    "            loss = loss_func(output, label.type(output.dtype))\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        print(index, f\"\\tloss = {loss.item()}\", \"time\", time.time() - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "We also define a validation function, `check_accuracy`, which tests a trained network on new data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def get_correctly_guessed_labels_function(\n",
    "    model: nn.Module, data: Tensor, labels: Tensor\n",
    ") -> int:\n",
    "    predictions = model(data)\n",
    "    list_of_predictions = [\n",
    "        round(prediction.type(torch.float).item()) for prediction in predictions\n",
    "    ]\n",
    "    correct = sum(\n",
    "        [\n",
    "            list_of_predictions[k] == labels.flatten().tolist()[k]\n",
    "            for k in range(len(predictions))\n",
    "        ]\n",
    "    )\n",
    "    return correct\n",
    "\n",
    "\n",
    "def _get_amount_of_labels(labels: Tensor) -> int:\n",
    "    # the first dimension of `labels` is `batch_size`\n",
    "    return labels.size(0)\n",
    "\n",
    "\n",
    "def check_accuracy(\n",
    "    network: nn.Module,\n",
    "    data_loader: DataLoader,\n",
    "    should_print: bool = True,\n",
    ") -> float:\n",
    "    num_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    network.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, labels in data_loader:\n",
    "            num_correct += get_correctly_guessed_labels_function(network, data, labels)\n",
    "            total += _get_amount_of_labels(labels)\n",
    "\n",
    "    accuracy = float(num_correct) / float(total)\n",
    "\n",
    "    if should_print:\n",
    "        print(f\"Test accuracy of the model: {accuracy*100:.2f}%\")\n",
    "        print(f\"num correct: {num_correct}, total: {total}\")\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "### Training and Verifying the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "For convenience, we load a pre-trained model and set the epoch size to 1. Training a network takes around 30 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment out for training\n",
    "my_network.load_state_dict(torch.load(\"trained_model.pth\"))\n",
    "num_epoch = 1\n",
    "# uncomment out for training\n",
    "# epoch=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \tloss = 0.09676678478717804 time 169.12708473205566\n"
     ]
    }
   ],
   "source": [
    "data_loader = training_dataloader\n",
    "train(my_network, training_dataloader, loss_func, optimizer, epoch=num_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy of the model: 97.07%\n",
      "num correct: 497, total: 512\n"
     ]
    }
   ],
   "source": [
    "accuracy = check_accuracy(my_network, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 9
}
