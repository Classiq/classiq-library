{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b144e47f-089a-49d4-a033-dbe6dcc7a55e",
   "metadata": {},
   "source": [
    "##### \n",
    "Quantum Vision Transformer Tutorial:\n",
    "##### 1. Description of Quantum Vision Transformer Architecture:\n",
    "<p> Quantum Vision Transformer is SOTA (state of the art) neural network architecture that works on image data. \n",
    " It was shown in numerous works that the quantum vision transformer can outperform own classical counterpart.\n",
    "This tutorial demonstrates the implementation of the hybrid architecture Quantum Vision Transformer [1][2], where some of the operation is quantum (like a Linear layer, Attention Layer) and some are classical.\n",
    "</p>\n",
    "\n",
    "\n",
    "##### 2. Quantum operations:\n",
    "###### 2.1 Angle Encoding.\n",
    "###### To feed the input data into the quantum circuit, we need to decode it using the angle encoding procedure; all input tensors are expanded at used as an angle for the rotation operation.\n",
    "###### 2.2 Quantum Layer:\n",
    "###### The quantum layer is constructed using the Rotation Operator acting on each wire, followed by the CNOT gate.\n",
    "\n",
    "###### 2.3 Attention mechanism:\n",
    "\n",
    "<p> It works with the sequence representation of patched images and utilizes the Attention mechanism that is the backbone of the Transformer architecture family. The multihead attention block is traditional for the transformer architecture; for the quantumness, the classical block is replaced with VQC. Another way to compute the attention is to use the quantum orthogonal layer, and calculate the dot product of i-th and j-th vectors.\n",
    "</p>\n",
    "\n",
    "##### 3. Classical operations:\n",
    "###### 3.1 Positional Encoder - the operation for the positional information of image patches incorporation.\n",
    "###### 3.3 FFN - Fully Connected Block, consists of MLP and LayerNormalization, followed by the residual connection.\n",
    "######  3.4. Transformer Block\n",
    "##### 4. Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb8f88e1-d493-4473-80cd-00042b61f868",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/baler/lib/python3.11/site-packages/classiq/_internals/authentication/token_manager.py:101: UserWarning: Device is already registered.\n",
      "Generating a new refresh token should only be done if the current refresh token is compromised.\n",
      "To do so, set the overwrite parameter to true\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import classiq\n",
    "classiq.authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20d0b3d8-8de2-460b-93d2-bc805561c0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/baler/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/opt/anaconda3/envs/baler/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <CFED5F8E-EC3F-36FD-AAA3-2C6C7F8D3DD9> /opt/anaconda3/envs/baler/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Expected in:     <CFB4297E-118A-3D27-BF39-55106327812A> /opt/anaconda3/envs/baler/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "import math\n",
    "from classiq import *\n",
    "from classiq import (\n",
    "    synthesize,\n",
    "    qfunc,\n",
    "    QArray,\n",
    "    QBit,\n",
    "    RX,\n",
    "    CArray,\n",
    "    Output,\n",
    "    CReal,\n",
    "    repeat,\n",
    "    create_model,\n",
    "    show\n",
    ")\n",
    "from classiq.execution import execute_qnn\n",
    "from classiq.applications.qnn import QLayer\n",
    "from classiq.qmod.symbolic import pi\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from classiq.execution import (\n",
    "    ExecutionPreferences,\n",
    "    execute_qnn,\n",
    "    set_quantum_program_execution_preferences,\n",
    ")\n",
    "from classiq.synthesis import SerializedQuantumProgram\n",
    "from classiq.applications.qnn.types import (\n",
    "    MultipleArguments,\n",
    "    ResultsCollection,\n",
    "    SavedResult,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91d168c1-d9da-4c10-a728-a179f3abc9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_QUBITS = 4\n",
    "num_shots = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c9b4ed5-6da6-491c-87eb-5fe7e0029304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"axioms-13-00323-g004-550.jpg\" width=\"800\" height=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image \n",
    "  \n",
    "# get the image \n",
    "Image(url=\"axioms-13-00323-g004-550.jpg\", width=800, height=400) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "407ef1c0-0a59-4821-911d-d6cb5827181d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"axioms-13-00323-g005-550.jpg\" width=\"800\" height=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"axioms-13-00323-g005-550.jpg\", width=800, height=300) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d172b4f-c317-4759-ad63-74b2eb4fa87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute(\n",
    "    quantum_program: SerializedQuantumProgram, arguments: MultipleArguments\n",
    ") -> ResultsCollection:\n",
    "    quantum_program = set_quantum_program_execution_preferences(\n",
    "        quantum_program, preferences=ExecutionPreferences(num_shots=num_shots)\n",
    "    )\n",
    "    return execute_qnn(quantum_program, arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b98e5452-3bc8-4aa0-beee-cc6d0c3cb994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(result: SavedResult) -> torch.Tensor:\n",
    "    res = result.value\n",
    "    yvec = [\n",
    "        (res.counts_of_qubits(k)[\"1\"] if \"1\" in res.counts_of_qubits(k) else 0)\n",
    "        / num_shots\n",
    "        for k in range(N_QUBITS)\n",
    "    ]\n",
    "\n",
    "    return torch.tensor(yvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2bb9e435-d050-468a-a61f-0d14ce16eb9c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_circuit():\n",
    "\n",
    "    #This function produces the quantum circuit:\n",
    "    @qfunc\n",
    "    def vqc(weight_: CArray[CArray[CReal, N_QUBITS], N_QUBITS], res:QArray) -> None:\n",
    "        \n",
    "        num_qubits = N_QUBITS\n",
    "        num_qlayers = N_QUBITS\n",
    "        \n",
    "        repeat(\n",
    "            count=num_qlayers,\n",
    "            iteration=lambda i: repeat(count=num_qubits,  iteration=lambda j: RX(pi * weight_[i][j], res[j]))\n",
    "        )\n",
    "        \n",
    "        repeat(\n",
    "            count=num_qubits - 1,\n",
    "            iteration=lambda index: CX(ctrl=res[index], target=res[index + 1]),\n",
    "        )\n",
    "        \n",
    "        CX(ctrl=res[num_qubits-1], target=res[0])\n",
    "\n",
    "    \n",
    "    \n",
    "    @qfunc\n",
    "    def main(input_: CArray[CReal, N_QUBITS], weight_: CArray[CArray[CReal, N_QUBITS], N_QUBITS], res: Output[QArray[QBit, N_QUBITS]]) -> None:\n",
    "        \n",
    "\n",
    "        encode_in_angle(input_, res)\n",
    "        vqc(weight_, res)\n",
    "\n",
    "\n",
    "    qmod = create_model(main)\n",
    "    quantum_program  = synthesize(qmod)\n",
    "    return quantum_program\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00f92a77-0b94-4fe6-8f9b-2331d4f77939",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patchify(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Patchify layer implemented using the Conv2d layer\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels:int, patch_size:int, hidden_size:int):\n",
    "        super(Patchify, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.conv = torch.nn.Conv2d(in_channels=in_channels, out_channels=hidden_size, kernel_size=self.patch_size, stride=self.patch_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        bs, c, h, w = x.size()\n",
    "        self.num_patches = (h // self.patch_size) ** 2\n",
    "\n",
    "        x = self.conv(x)\n",
    "        x = x.view(bs, self.num_patches, self.hidden_size)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6280490e-86b2-4023-bd59-8ded8ca43907",
   "metadata": {},
   "source": [
    "#### Rotary Positional Embedding:\n",
    "#### $$f_{q, k}(x,m) = R^{d}_{\\theta, m} W_{q,x}x_{m}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2849e3a5-3d01-42bf-ae4b-4c79375e8443",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryPositionalEmbedding(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Rotary Positional Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_seq_len):\n",
    "        super(RotaryPositionalEmbedding, self).__init__()\n",
    "\n",
    "        # Create a rotation matrix.\n",
    "        self.rotation_matrix = torch.zeros(d_model, d_model)\n",
    "        for i in range(d_model):\n",
    "            for j in range(d_model):\n",
    "                self.rotation_matrix[i, j] = math.cos(i * j * 0.01)\n",
    "\n",
    "        # Create a positional embedding matrix.\n",
    "        self.positional_embedding = torch.zeros(max_seq_len, d_model)\n",
    "        for i in range(max_seq_len):\n",
    "            for j in range(d_model):\n",
    "                self.positional_embedding[i, j] = math.cos(i * j * 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: A tensor of shape (batch_size, seq_len, d_model).\n",
    "\n",
    "        Returns:\n",
    "            A tensor of shape (batch_size, seq_len, d_model).\n",
    "        \"\"\"\n",
    "\n",
    "        # Add the positional embedding to the input tensor.\n",
    "        x += self.positional_embedding\n",
    "\n",
    "        # Apply the rotation matrix to the input tensor.\n",
    "        x = torch.matmul(x, self.rotation_matrix)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d99e78-0e5b-463c-a5eb-891c8fc35c1d",
   "metadata": {},
   "source": [
    "##### Quantum Layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9711b490-835e-4e29-9567-72e4a8f2fe09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"classiq_circuit.png\" width=\"800\" height=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"classiq_circuit.png\", width=800, height=300) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19042677-6d63-426c-b12d-9d107b47eecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumLayer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Quantum Layer\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(QuantumLayer, self).__init__()\n",
    "        self.quantum_program = get_circuit()\n",
    "        self.quantum_layer = QLayer(self.quantum_program, execute_qnn, post_process)\n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        x = self.quantum_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da820af8",
   "metadata": {},
   "source": [
    "##### Feed Forward Neural Network:\n",
    " $$f_{i}(X) =  GELU \\circ Dropout \\circ QuantumLayer(X)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "376d717d-76bf-45bc-a90f-67d15a92f33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Feed Forward Network\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, hidden_size):\n",
    "        super().__init__()\n",
    "        self.qlinear = QuantumLayer(hidden_size, hidden_size)\n",
    "        self.dropout = torch.nn.Dropout(p=0.4)\n",
    "        return\n",
    "    \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        seq_len = x.size()[1]\n",
    "        #x = self.linear_1(x)\n",
    "        x = [self.qlinear(x[:, t, :]) for t in range(seq_len)]\n",
    "        x = torch.Tensor(pad_sequence(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.nn.functional.gelu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf5bc46",
   "metadata": {},
   "source": [
    "#### Multihead Attention:\n",
    "#### $$Attention = softmax(\\frac{K(X)*Q(X)^T}{\\sqrt{dim}})*V(X)$$, where K, Q, V is the quantum Linear Projection of the input data;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2ffa889-38a6-425b-bf76-c1032406e6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class qMHA(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Quantum Multihead Attention\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim:int, num_heads:int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.k_linear = QuantumLayer(in_dim, in_dim);\n",
    "        self.q_linear = QuantumLayer(in_dim, in_dim);\n",
    "        self.v_linear = QuantumLayer(in_dim, in_dim);\n",
    "        self.dropout = torch.nn.Dropout(p=0.1)\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.in_dim = in_dim\n",
    "        \n",
    "        return\n",
    "\n",
    "    def forward(self, X:torch.Tensor):\n",
    "\n",
    "        seq_len = X.size()[1]\n",
    "        K = [self.k_linear(X[:, t, :]) for t in range(seq_len)]\n",
    "        Q = [self.q_linear(X[:, t, :]) for t in range(seq_len)]\n",
    "        V = [self.v_linear(X[:, t, :]) for t in range(seq_len)]\n",
    "        \n",
    "        k = torch.Tensor(pad_sequence(K))\n",
    "        q = torch.Tensor(pad_sequence(Q))\n",
    "        v = torch.Tensor(pad_sequence(V))\n",
    "    \n",
    "        attention = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        attention = torch.nn.functional.softmax(attention, dim=-1)\n",
    "\n",
    "        attention = self.dropout(attention)\n",
    "        attention = attention @ v \n",
    "        #x = self.final_l(attention)\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7e14e4",
   "metadata": {},
   "source": [
    "#### Transformer Encoder Block:\n",
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86af5bc7-f121-4487-b5dc-55dfaa5508e9",
   "metadata": {},
   "source": [
    "$$\n",
    " \\begin{equation}\n",
    "    \\begin{cases}\n",
    "     f_{i-1}(x) = X + GELU\\circ Linear \\circ Dropout \\circ QuantumLinear \\circ Linear \\circ X\\\\\n",
    "    f_{i}(x) = f_{i-1}(X) + GELU \\circ Linear \\circ Dropout \\circ QuantumAttention \\circ Linear \\circ f_{i-1}(X)\n",
    " \\end{cases}\n",
    " \\end{equation}\n",
    " $$\n",
    "GELU is an activation function. Linear is the linear projection of the input tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "426e1bed-2197-4d62-ba13-6215c466c97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class qTransformerEncoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Quantum Transformer Encoder Layer\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim:int, num_heads:int) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer_norm_1 = torch.nn.LayerNorm(normalized_shape=in_dim)\n",
    "        self.layer_norm_2 = torch.nn.LayerNorm(normalized_shape=in_dim)\n",
    "        \n",
    "        self.qMHA = qMHA(in_dim, num_heads)\n",
    "        self.qFFN = FFN(in_dim, hidden_size=in_dim)\n",
    "        self.dropout = torch.nn.Dropout(p=0.1)\n",
    "        \n",
    "\n",
    "    def forward(self, X:torch.Tensor):\n",
    "        x = self.qMHA(X)\n",
    "        \n",
    "        x = (self.layer_norm_1(x) + X)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        y = self.qFFN(x)\n",
    "        y = self.layer_norm_2(y)+x\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b125354b",
   "metadata": {},
   "source": [
    "#### Quantum Vision Transformer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f757f693-5d23-4ae7-835f-055c9bc06f6c",
   "metadata": {},
   "source": [
    "$$\n",
    " \\begin{equation}\n",
    "     \\begin{cases}\n",
    "     X = Patrchify(X)\\\\\n",
    "     X = PositionalEncoding (X)\\\\\n",
    "     X = TransformerEncoder(X)\\\\\n",
    "     X = Mean(X)\\\\\n",
    "     X = Softmax(X)\n",
    "    \\end{cases}\n",
    " \\end{equation}\n",
    " $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6a37ec6-d82c-4784-aa97-94bf80be78c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QVT(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Quantum Vision Transformer;\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, patch_size, in_dim, hidden_size,  num_heads, n_classes, n_layers) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = (in_dim//patch_size)**2\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.patch_formation = Patchify(in_channels=in_channels, patch_size=patch_size, hidden_size=hidden_size)\n",
    "\n",
    "        self.pos_encoding = RotaryPositionalEmbedding(hidden_size, self.d_model)\n",
    "        self.transformer_blocks = torch.nn.ModuleList([qTransformerEncoder(hidden_size, num_heads) for i in range(n_layers)])\n",
    "                \n",
    "        self.final_normalization = torch.nn.LayerNorm(hidden_size)\n",
    "        self.final_layer = torch.nn.Linear(hidden_size, self.n_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:  \n",
    "        \n",
    "        x = self.patch_formation(x)\n",
    "        x += self.pos_encoding(x)\n",
    "        \n",
    "        for trans_block in self.transformer_blocks:\n",
    "            x = trans_block(x)\n",
    "        \n",
    "        x = self.final_normalization(x)\n",
    "        x = x.mean(axis=1)\n",
    "        x = self.final_layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9038db",
   "metadata": {},
   "source": [
    "#### Definition of MNIST dataset and dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79282b7f-cb02-4b20-a29a-51de390c2d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Example with the MNIST Dataset:\n",
    "transform=transforms.Compose([\n",
    "                          transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
    "                          transforms.Normalize((0.1307,), (0.3081,)) # normalize inputs\n",
    "                      ])\n",
    "dataset1 = datasets.MNIST('../data', train=True, download=True,transform=transform)\n",
    "dataset2 = datasets.MNIST('../data', train=False,transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,batch_size=256)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2,batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f94e4047-abcd-4d48-ab16-e1442474f6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Classifier and optimizer definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ed4f759",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = QVT(in_channels=1, patch_size=7, in_dim=28, hidden_size=4, num_heads=1, n_classes=10, n_layers=1)\n",
    "\n",
    "opt = optim.SGD(clf.parameters(), lr=0.001, momentum=0.5)\n",
    "\n",
    "loss_history = []\n",
    "acc_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f70853",
   "metadata": {},
   "source": [
    "#### Training Procedure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc61ea1e-0cfa-4498-b5ec-eab29b102750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    clf.train() # set model in training mode (need this because of dropout)\n",
    "    \n",
    "    # dataset API gives us pythonic batching \n",
    "    for data, label in tqdm.tqdm(train_loader):\n",
    "        opt.zero_grad()\n",
    "        preds = clf(data)\n",
    "        loss = torch.nn.functional.nll_loss(preds, label)\n",
    "        loss.backward()\n",
    "        loss_history.append(loss)\n",
    "        opt.step()\n",
    "    return loss_history\n",
    "\n",
    "def test():\n",
    "    clf.eval() # set model in inference mode (need this because of dropout)\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for data, target in tqdm.tqdm(test_loader):\n",
    "        \n",
    "        output = clf(data)\n",
    "        test_loss += torch.nn.functional.nll_loss(output, target).item()\n",
    "        pred = output.argmax() # get the index of the max log-probability\n",
    "        correct += pred.eq(target).cpu().sum()\n",
    "\n",
    "    test_loss = test_loss\n",
    "    test_loss /= len(test_loader) # loss function already averages over batch size\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    acc_history.append(accuracy)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c0281a-c8e9-46c8-adbf-dfdce58ddc61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/235 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "#for epoch in range(0, 3):\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372d1033-5fb1-486a-a2bd-8d3fd52fffcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4a83d3-909c-49f5-a2e4-e1ae9bfcc5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "plt.title('Model Loss')\n",
    "plt.plot(range(len(loss_history)), [i.detach().numpy() for i in loss_history], label=\"training\")\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38170056",
   "metadata": {},
   "source": [
    "##### List of refferences:\n",
    "##### [1] Quantum Vision Transformer. https://arxiv.org/pdf/2209.08167\n",
    "##### [2] Quantum Vision Transformers for Quark–Gluon Classification. https://arxiv.org/pdf/2405.10284\n",
    "##### [3] Quantum Attention for Vision Transformer in High-Energy Physics. https://arxiv.org/pdf/2411.13520\n",
    "##### [4] Quantum Vision Transformers for Quark–Gluon Classification. https://indico.jlab.org/event/459/papers/11832/files/1318-First_Measurements_With_A_Quantum_Vision_Transformer_A_Naive_Approach_IEEE__CHEP_refereeEdits.pdf\n",
    "##### [5] Quantum Vision Transformers for Quark–Gluon Classification. https://arxiv.org/pdf/2405.10284\n",
    "##### [6] Quantum Mixed-State Self-Attention Network. https://arxiv.org/html/2403.02871v1\n",
    "##### [7] Quantum Self-Attention Neural Networks for Text Classification. https://arxiv.org/pdf/2205.05625"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
