{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb8f88e1-d493-4473-80cd-00042b61f868",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/baler/lib/python3.11/site-packages/classiq/_internals/authentication/token_manager.py:101: UserWarning: Device is already registered.\n",
      "Generating a new refresh token should only be done if the current refresh token is compromised.\n",
      "To do so, set the overwrite parameter to true\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import classiq\n",
    "classiq.authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20d0b3d8-8de2-460b-93d2-bc805561c0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/baler/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/opt/anaconda3/envs/baler/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <CFED5F8E-EC3F-36FD-AAA3-2C6C7F8D3DD9> /opt/anaconda3/envs/baler/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Expected in:     <CFB4297E-118A-3D27-BF39-55106327812A> /opt/anaconda3/envs/baler/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from typing import List\n",
    "import pdb\n",
    "\n",
    "from classiq import (\n",
    "    synthesize,\n",
    "    qfunc,\n",
    "    QArray,\n",
    "    QBit,\n",
    "    RX,\n",
    "    CArray,\n",
    "    Output,\n",
    "    CReal,\n",
    "    allocate,\n",
    "    repeat,\n",
    "    create_model,\n",
    "    show\n",
    ")\n",
    "\n",
    "# choosing our data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import math\n",
    "from classiq.applications.qnn.types import SavedResult\n",
    "from classiq.applications.qnn import QLayer\n",
    "from classiq.qmod.symbolic import pi\n",
    "from classiq.synthesis import SerializedQuantumProgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "634a20e6-3b46-4907-84f7-27ab3f473417",
   "metadata": {},
   "outputs": [],
   "source": [
    "from classiq.execution import (\n",
    "    ExecutionPreferences,\n",
    "    execute_qnn,\n",
    "    set_quantum_program_execution_preferences,\n",
    ")\n",
    "from classiq.synthesis import SerializedQuantumProgram\n",
    "from classiq.applications.qnn.types import (\n",
    "    MultipleArguments,\n",
    "    ResultsCollection,\n",
    "    SavedResult,\n",
    ")\n",
    "\n",
    "\n",
    "from classiq.applications.qnn.types import (\n",
    "    MultipleArguments,\n",
    "    ResultsCollection,\n",
    "    SavedResult,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d168c1-d9da-4c10-a728-a179f3abc9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_QUBITS = 4\n",
    "num_shots = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d172b4f-c317-4759-ad63-74b2eb4fa87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute(\n",
    "    quantum_program: SerializedQuantumProgram, arguments: MultipleArguments\n",
    ") -> ResultsCollection:\n",
    "    quantum_program = set_quantum_program_execution_preferences(\n",
    "        quantum_program, preferences=ExecutionPreferences(num_shots=num_shots)\n",
    "    )\n",
    "    return execute_qnn(quantum_program, arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b98e5452-3bc8-4aa0-beee-cc6d0c3cb994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(result: SavedResult) -> torch.Tensor:\n",
    "    res = result.value\n",
    "    yvec = [\n",
    "        (res.counts_of_qubits(k)[\"1\"] if \"1\" in res.counts_of_qubits(k) else 0)\n",
    "        / num_shots\n",
    "        for k in range(N_QUBITS)\n",
    "    ]\n",
    "\n",
    "    return torch.tensor(yvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bb9e435-d050-468a-a61f-0d14ce16eb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_circuit():\n",
    "\n",
    "    @qfunc\n",
    "    def vqc(input_: CArray[CReal, N_QUBITS], weight_: CArray[CArray[CReal, N_QUBITS], N_QUBITS], res:  QArray[QBit]) -> None:\n",
    "        num_qubits = input_.len\n",
    "        num_qlayers = weight_.len\n",
    "        repeat(\n",
    "            count=num_qlayers,\n",
    "            iteration=lambda i: repeat(count=num_qubits, \n",
    "                                           iteration=lambda j:RX(pi * weight_[i][j], res[j]))\n",
    "        )\n",
    "        \n",
    "    @qfunc\n",
    "    def angle_embedding(input_: CArray[CReal, N_QUBITS], res: QArray[QBit]) -> None:\n",
    "        repeat(\n",
    "            count=input_.len,\n",
    "            iteration=lambda index: RX(pi * input_[index], res[index]),\n",
    "        )\n",
    "\n",
    "    @qfunc\n",
    "    def main(input_: CArray[CReal, N_QUBITS], weight_: CArray[CArray[CReal, N_QUBITS], N_QUBITS], res: Output[QArray[QBit]]) -> None:\n",
    "        allocate(N_QUBITS, res)\n",
    "        angle_embedding(input_=input_, res=res)\n",
    "        vqc(input_=input_, weight_=weight_, res=res)\n",
    "        \n",
    "    qmod = create_model(main)\n",
    "    quantum_program  = synthesize(qmod)\n",
    "    return quantum_program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00f92a77-0b94-4fe6-8f9b-2331d4f77939",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patchify(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Patchify layer implemented using the Conv2d layer\n",
    "    \"\"\"\n",
    "    def __init__(self, patch_size:int, hidden_size:int):\n",
    "        super(Patchify, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.conv = torch.nn.Conv2d(in_channels=3, out_channels=hidden_size, kernel_size=self.patch_size, stride=self.patch_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        bs, c, h, w = x.size()\n",
    "        self.num_patches = (h // self.patch_size) ** 2\n",
    "\n",
    "        x = self.conv(x)\n",
    "        x = x.view(bs, self.num_patches, self.hidden_size)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2849e3a5-3d01-42bf-ae4b-4c79375e8443",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryPositionalEmbedding(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Rotary Positional Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_seq_len):\n",
    "        super(RotaryPositionalEmbedding, self).__init__()\n",
    "\n",
    "        # Create a rotation matrix.\n",
    "        self.rotation_matrix = torch.zeros(d_model, d_model)\n",
    "        for i in range(d_model):\n",
    "            for j in range(d_model):\n",
    "                self.rotation_matrix[i, j] = math.cos(i * j * 0.01)\n",
    "\n",
    "        # Create a positional embedding matrix.\n",
    "        self.positional_embedding = torch.zeros(max_seq_len, d_model)\n",
    "        for i in range(max_seq_len):\n",
    "            for j in range(d_model):\n",
    "                self.positional_embedding[i, j] = math.cos(i * j * 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: A tensor of shape (batch_size, seq_len, d_model).\n",
    "\n",
    "        Returns:\n",
    "            A tensor of shape (batch_size, seq_len, d_model).\n",
    "        \"\"\"\n",
    "\n",
    "        # Add the positional embedding to the input tensor.\n",
    "        x += self.positional_embedding\n",
    "\n",
    "        # Apply the rotation matrix to the input tensor.\n",
    "        x = torch.matmul(x, self.rotation_matrix)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19042677-6d63-426c-b12d-9d107b47eecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumLayer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Quantum Layer\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(QuantumLayer, self).__init__()\n",
    "        self.quantum_program = get_circuit()\n",
    "        self.quantum_layer = QLayer(self.quantum_program, execute, post_process)\n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        size = x.size()\n",
    "        x = x.view(-1, size[-1])\n",
    "        x = self.quantum_layer(x)\n",
    "        x = x.view(size)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "376d717d-76bf-45bc-a90f-67d15a92f33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Feed Forward Network\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, hidden_size):\n",
    "        super().__init__()\n",
    "        self.linear_1 = torch.nn.Linear(in_dim, hidden_size)\n",
    "        self.qlinear_1 = QuantumLayer(hidden_size, hidden_size)\n",
    "        self.dropout = torch.nn.Dropout(p=0.4)\n",
    "        self.linear_2 = torch.nn.Linear(hidden_size, in_dim)\n",
    "        return\n",
    "    \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        x = self.linear_1(x)\n",
    "        x = self.qlinear_1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.nn.functional.gelu(x)\n",
    "        x = self.linear_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2ffa889-38a6-425b-bf76-c1032406e6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class qMHA(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Quantum Multihead Attention\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim:int, num_heads:int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.qK = QuantumLayer(in_dim, in_dim);\n",
    "        self.qQ = QuantumLayer(in_dim, in_dim);\n",
    "        self.qV = QuantumLayer(in_dim, in_dim);\n",
    "        self.dropout = torch.nn.Dropout(p=0.1)\n",
    "        \n",
    "        self.final_l = QuantumLayer(in_dim, in_dim)\n",
    "        self.num_heads = num_heads\n",
    "        self.in_dim = in_dim\n",
    "        \n",
    "        return\n",
    "\n",
    "    def forward(self, X:torch.Tensor):\n",
    "\n",
    "        dim = torch.sqrt(torch.Tensor([X.shape[-1]]))\n",
    "        attention = torch.nn.functional.softmax((1/dim)*self.qK(X))*self.qQ(X)*self.qV(X)\n",
    "        x = self.dropout(attention)\n",
    "        x = self.final_l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "426e1bed-2197-4d62-ba13-6215c466c97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class qTransformerEncoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Quantum Transformer Encoder Layer\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim:int, num_heads:int) -> None:\n",
    "        super().__init__()\n",
    "        self.layer_norm_1 = torch.nn.LayerNorm(normalized_shape=in_dim)\n",
    "        self.layer_norm_2 = torch.nn.LayerNorm(normalized_shape=in_dim)\n",
    "        \n",
    "        self.qMHA = qMHA(in_dim, num_heads)\n",
    "        self.qFFN = FFN(in_dim, hidden_size=in_dim)\n",
    "        self.dropout = torch.nn.Dropout(p=0.3)\n",
    "        \n",
    "\n",
    "    def forward(self, X:torch.Tensor):\n",
    "        \n",
    "        x = self.qMHA(X)\n",
    "        x = self.layer_norm_1(x)\n",
    "        x = self.dropout(x) + X\n",
    "\n",
    "        y = self.layer_norm_2(x)\n",
    "        y = self.qFFN(y)+y\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6a37ec6-d82c-4784-aa97-94bf80be78c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QVT(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Quantum Vision Transformer;\n",
    "    \"\"\"\n",
    "    def __init__(self, patch_size, in_dim, hidden_size,  num_heads, n_classes, n_layers) -> None:\n",
    "        super().__init__()\n",
    "        self.patch_formation = Patchify(patch_size=patch_size, hidden_size=hidden_size)\n",
    "        self.d_model = (in_dim//patch_size)**2\n",
    "        self.pos_encoding = RotaryPositionalEmbedding(hidden_size, self.d_model)\n",
    "        self.transformer_blocks = [qTransformerEncoder(hidden_size, num_heads) for i in range(n_layers)]\n",
    "        \n",
    "        self.n_classes = n_classes\n",
    "        self.final_normalization = torch.nn.LayerNorm(hidden_size)\n",
    "        self.final_layer = torch.nn.Linear(hidden_size, self.n_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:        \n",
    "        x = self.patch_formation(x)\n",
    "        x += self.pos_encoding(x)\n",
    "        for trans_block in self.transformer_blocks:\n",
    "            x = trans_block(x)\n",
    "        x = self.final_normalization(x)\n",
    "        x = x.mean(axis=1)\n",
    "        x = self.final_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b57b43fe-0963-4576-95f3-55e7655858e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zj/jmm9ywc51bb4p1pn12_23y340000gn/T/ipykernel_4329/535465168.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attention = torch.nn.functional.softmax((1/dim)*self.qK(X))*self.qQ(X)*self.qV(X)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(32, 3, 16, 16)\n",
    "qVisTransformer = QVT(patch_size=4, in_dim=16, hidden_size=4, num_heads=1, n_classes=10, n_layers=4)\n",
    "res = qVisTransformer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86242cc3-2d4d-48e2-83c2-1ea397976faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79282b7f-cb02-4b20-a29a-51de390c2d17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
