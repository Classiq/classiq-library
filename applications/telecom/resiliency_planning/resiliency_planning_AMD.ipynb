{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Quantum-Based Resiliency Planning\n",
    "\n",
    "This notebook implements the quantum-based resiliency planning using QAOA. This instance of the notebook does not use the Classiq built-in simulator but a specific build of the Qiskit Aer simulator for AMD GPUs. To run this notebook, make sure that you are running this with a AMD GPU otherwise you are not able to build and run the full notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0228ed-4175-4d66-ab8c-07f97928bbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# set environment variables\n",
    "export ROCM_PATH=/opt/rocm\n",
    "export AER_THRUST_BACKEND=ROCM\n",
    "export QISKIT_AER_PACKAGE_NAME=qiskit-aer-gpu-rocm\n",
    "export ROCR_VISIBLE_DEVICES=0\n",
    "export HIP_VISIBLE_DEVICES=0\n",
    "ulimit -s unlimited\n",
    "\n",
    "## Get Git\n",
    "apt-get update && apt-get install -y git ninja-build\n",
    "\n",
    "# pull rocm-qiskit-aer branch\n",
    "mkdir quantum-sim  && cd quantum-sim/\n",
    "git clone https://github.com/coketaste/qiskit-aer.git\n",
    "cd qiskit-aer/\n",
    "git switch coketaste/amd-rocm-mi300\n",
    "\n",
    "\n",
    "# build rocm-qiskit-aer branch\n",
    "rm -rf _skbuild dist build\n",
    "pip install cmake pybind11 \"conan<2\" scikit-build\n",
    "pip install -r requirements-dev.txt\n",
    "python3 setup.py bdist_wheel -- \\\n",
    "  -DCMAKE_CXX_COMPILER=/opt/rocm/llvm/bin/clang++ \\\n",
    "  -DCMAKE_HIP_COMPILER=/opt/rocm/llvm/bin/clang++ \\\n",
    "  -DAER_THRUST_BACKEND=ROCM\n",
    "\n",
    "\n",
    "pip install --force-reinstall dist/qiskit_aer_gpu_rocm*.whl\n",
    "pip install qiskit\n",
    "\n",
    "\n",
    "# run sample benchmark\n",
    "examples/single_gpu/benchmark.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from itertools import product\n",
    "from platform import node\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "from tqdm import tqdm\n",
    "\n",
    "from classiq import *\n",
    "from classiq.execution import ExecutionPreferences, ExecutionSession\n",
    "\n",
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Create the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Graph ----\n",
    "graph = nx.Graph()\n",
    "V = [\"P1\", \"P2\", \"S1\", \"S2\", \"S3\"]\n",
    "graph.add_nodes_from(V)\n",
    "\n",
    "E_with_weights = [\n",
    "    (\"P1\", \"S1\", {\"lat\": 1, \"perr\": 0.1}),\n",
    "    (\"P1\", \"S2\", {\"lat\": 1, \"perr\": 0.1}),\n",
    "    (\"S2\", \"S1\", {\"lat\": 1, \"perr\": 0.1}),\n",
    "    (\"P2\", \"S1\", {\"lat\": 1, \"perr\": 0.1}),\n",
    "    (\"S3\", \"S1\", {\"lat\": 1, \"perr\": 0.1}),\n",
    "    (\"P2\", \"S3\", {\"lat\": 1, \"perr\": 0.1}),\n",
    "    (\"S3\", \"S2\", {\"lat\": 2, \"perr\": 0.1}),\n",
    "]\n",
    "\n",
    "lat_vec = np.array([w[\"lat\"] for (_, _, w) in E_with_weights], dtype=float)\n",
    "perr_vec = np.array([w[\"perr\"] for (_, _, w) in E_with_weights], dtype=float)\n",
    "graph.add_edges_from(E_with_weights)\n",
    "\n",
    "E = graph.edges()\n",
    "lat = {(u, v): w[\"lat\"] for (u, v, w) in E_with_weights}\n",
    "perr = {(u, v): w[\"perr\"] for (u, v, w) in E_with_weights}\n",
    "ordered_edges = [(u, v) for (u, v, _) in E_with_weights]\n",
    "edge_ids = [f\"({u},{v})\" for (u, v) in ordered_edges]\n",
    "ids = edge_ids + V\n",
    "\n",
    "\n",
    "def weight_of_e(u, v, weights):\n",
    "    if (u, v) in lat.keys():\n",
    "        return weights[(u, v)]\n",
    "    return weights[(v, u)]\n",
    "\n",
    "\n",
    "def lat_of_e(u, v):\n",
    "    return weight_of_e(u, v, lat)\n",
    "\n",
    "\n",
    "def perr_of_e(u, v):\n",
    "    return weight_of_e(u, v, perr)\n",
    "\n",
    "\n",
    "# ---- Demands ----\n",
    "# From S2\n",
    "source_node = \"S2\"\n",
    "demands = [\n",
    "    {\"name\": \"d1\", \"s\": source_node, \"t\": \"P1\"},\n",
    "    {\"name\": \"d2\", \"s\": source_node, \"t\": \"P2\"},\n",
    "]\n",
    "\n",
    "D = len(demands)\n",
    "m = len(E)\n",
    "n = len(V)\n",
    "\n",
    "# ---- Z 2D matrix ----\n",
    "row_len = m + n\n",
    "Z = np.zeros((D, (m + n)), dtype=int)\n",
    "\n",
    "# Creating the solution we aim to get\n",
    "Z[0, 1] = Z[0, 7] = Z[0, 10] = 1\n",
    "Z[1, 5] = Z[1, 6] = Z[1, 11] = Z[1, 10] = Z[1, 8] = 1\n",
    "\n",
    "\n",
    "Z_df = pd.DataFrame(\n",
    "    Z, index=[f\"{d['name']}({d['s']}→{d['t']})\" for d in demands], columns=ids\n",
    ")\n",
    "\n",
    "# ---- Plot ----\n",
    "pos = {\n",
    "    \"P1\": (2.0, 1.0),\n",
    "    \"P2\": (1.0, 1.0),\n",
    "    \"S1\": (1.5, 1.0),\n",
    "    \"S2\": (1.75, 2.0),\n",
    "    \"S3\": (1.25, 2.0),\n",
    "}\n",
    "plt.figure(figsize=(7.5, 3.2))\n",
    "nx.draw(graph, pos, with_labels=True, node_size=1000)\n",
    "nx.draw_networkx_edge_labels(\n",
    "    graph, pos, edge_labels={(u, v): f\"{lat_of_e(u,v)}\" for (u, v) in E}\n",
    ")\n",
    "plt.title(\"Directed graph with 6 nodes and 7 edges (edge labels = latency)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\\nDemands (2 total):\")\n",
    "print(\n",
    "    pd.DataFrame(\n",
    "        [{\"Demand\": d[\"name\"], \"Source\": d[\"s\"], \"Target\": d[\"t\"]} for d in demands]\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\nZ (D x (m+n)) binary matrix — rows=demand, cols=edge (initialized to 0):\")\n",
    "print(Z_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_edges = len(ordered_edges)\n",
    "err_correlation = np.full((num_edges, num_edges), 0)\n",
    "correlated = True\n",
    "i = j = 0\n",
    "for index, (u1, v1) in enumerate(ordered_edges):\n",
    "    if (u1, v1) in [(\"P1\", \"S2\"), (\"S2\", \"P1\")]:\n",
    "        i = index\n",
    "    if (u1, v1) in [(\"S1\", \"S2\"), (\"S2\", \"S1\")]:\n",
    "        j = index\n",
    "assert i != j != 0\n",
    "if correlated:\n",
    "    err_correlation[i, j] = err_correlation[j, i] = 1\n",
    "err_correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "For convience and easy visualization, let's add a helper function that prints the solution into the graph.\n",
    "\n",
    "This function gets our assigned solution (the 2D array of binary variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def get_edge(u, v, arr):\n",
    "    if (u, v) in arr:\n",
    "        return (u, v)\n",
    "    return (v, u)\n",
    "\n",
    "\n",
    "def print_solution_graph(assigned_z, index=None):\n",
    "    # Assign a distinct color per demand\n",
    "    colors_for_demands = [\"red\", \"blue\", \"green\", \"yellow\"]\n",
    "    edge_colors = []\n",
    "    edge_widths = []\n",
    "    node_colors = []\n",
    "\n",
    "    # Build a mapping from edge -> demand index (if chosen)\n",
    "    edge_to_demand = {}\n",
    "    for d_idx, d in enumerate(demands):\n",
    "        for e_idx, (u, v) in enumerate(ordered_edges):\n",
    "            if assigned_z[d_idx, e_idx] == 1:\n",
    "                edge_to_demand[(u, v)] = d_idx\n",
    "\n",
    "    # Create color list for all edges in E\n",
    "    for u, v in E:\n",
    "        if (u, v) in edge_to_demand or (v, u) in edge_to_demand:\n",
    "            demand_idx = edge_to_demand[get_edge(u, v, edge_to_demand)]\n",
    "            edge_colors.append(colors_for_demands[demand_idx])\n",
    "            edge_widths.append(3.0)\n",
    "        else:\n",
    "            edge_colors.append(\"lightgray\")\n",
    "            edge_widths.append(1.0)\n",
    "\n",
    "    # Plot the graph\n",
    "    plt.figure(figsize=(8, 3.6))\n",
    "    nx.draw(\n",
    "        graph,\n",
    "        pos,\n",
    "        with_labels=True,\n",
    "        node_size=1000,\n",
    "        edge_color=edge_colors,\n",
    "        width=edge_widths,\n",
    "    )\n",
    "    nx.draw_networkx_edge_labels(\n",
    "        graph, pos, edge_labels={(u, v): f\"{lat_of_e(u,v)}\" for (u, v) in ordered_edges}\n",
    "    )\n",
    "    if index is not None:\n",
    "        plt.title(\n",
    "            f\"Routing solution for sample index {index} — edges colored by demand\"\n",
    "        )\n",
    "    else:\n",
    "        plt.title(\"Optimal routing solution — edges colored by demand\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(Z_df)\n",
    "print_solution_graph(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The mixer Hamiltonian is effectively X/2 and has eigenvalues -0.5 and +0.5. So the difference between minimum and maximum eigenvalues is exactly 1.\n",
    "# This can be rescaled by a global scaling parameter.\n",
    "GlobalScalingParameter = 1\n",
    "\n",
    "# The constraint Hamiltonian has the property that a minimal constraint violation is 1 and no constraint violation is 0.\n",
    "# We wish to normalise the constraint Hamiltonian relative to the total cost Hamiltonian such that the constraint violation will be 1~2 x larger than the maximal difference between total cost values.\n",
    "RelativeConstraintNormalisation = 5\n",
    "\n",
    "# The cost Hamiltonian should be similar in eigenvalue difference to the mixer Hamiltonian and should be normalised to about 1.\n",
    "# To find the exact normalisation requires solving this NP hard problem so we always use approximation.\n",
    "# Since this is approximate, there is a relative scaling parameter we can twitch\n",
    "RelativeCostNormalisation = 1 / 40\n",
    "\n",
    "TotalCostNormalisation = GlobalScalingParameter * RelativeCostNormalisation\n",
    "TotalConstraintNormalisation = RelativeConstraintNormalisation * TotalCostNormalisation\n",
    "\n",
    "# B stands for the relationship between error correlation and latency\n",
    "B = 10\n",
    "\n",
    "# Normalize latencies\n",
    "min_lat_guess = (\n",
    "    3  # can be calculated with Dijkstra when removing the single assignment constraint\n",
    ")\n",
    "max_lat_guess = 4  # any guess that fits the constraints will work\n",
    "\n",
    "lat_normalized = {}\n",
    "for e, w in lat.items():\n",
    "    lat_normalized[e] = w * TotalCostNormalisation / (max_lat_guess - min_lat_guess)\n",
    "\n",
    "min_prob_guess = 0.03\n",
    "max_prob_guess = 1.0\n",
    "\n",
    "print(lat_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edge_fidx(d_idx, e_idx):\n",
    "    return d_idx * row_len + e_idx\n",
    "\n",
    "\n",
    "def node_fidx(d_idx, n_idx):\n",
    "    return d_idx * row_len + n_idx + m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Define Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_per_array(assigned_z, array):\n",
    "    return sum(\n",
    "        (array[e] * assigned_z[edge_fidx(d_idx, e_idx)])\n",
    "        for d_idx, d in enumerate(demands)\n",
    "        for e_idx, e in enumerate(ordered_edges)\n",
    "    )\n",
    "\n",
    "\n",
    "# Objective: sum_d sum_e lat_e * Z[d,e]\n",
    "def objective_func(assigned_z):\n",
    "    lat_sum = sum_per_array(assigned_z, lat_normalized)\n",
    "    return lat_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Define Single Assignment Cosntraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edges_per_node(node):\n",
    "    for edge_idx, edge in enumerate(ordered_edges):\n",
    "        if node in edge:\n",
    "            yield edge_idx\n",
    "\n",
    "\n",
    "def flow_conservation_per_node_per_demand(\n",
    "    node, node_idx, demand_idx, demand, assigned_z\n",
    "):\n",
    "    node_flow = 0\n",
    "\n",
    "    # If node is the start/end of this demand, add 1 imaginary edge\n",
    "    if node in demand.values():\n",
    "        node_flow += 1\n",
    "\n",
    "    # If the node is in the path, subtract 2 required edges\n",
    "    node_flow -= 2 * assigned_z[node_fidx(d_idx=demand_idx, n_idx=node_idx)]\n",
    "\n",
    "    # Add 1 edge for each used edge for this node\n",
    "    for edge_idx in edges_per_node(node):\n",
    "        node_flow += assigned_z[edge_fidx(d_idx=demand_idx, e_idx=edge_idx)]\n",
    "\n",
    "    # Valid solution should have node_flow == 0 since:\n",
    "    # * For nodes not in the path - no edges should be chosen\n",
    "    # * For nodes in the path, there should be exactly 2 edges, or 1 edge for start/end ( + imaginary edge)\n",
    "    return node_flow**2\n",
    "\n",
    "\n",
    "def my_not(value):\n",
    "    # Assumes value is 0 or 1\n",
    "    return 1 - value\n",
    "\n",
    "\n",
    "def constraint_starting_nodes(assigned_z):\n",
    "    return sum(\n",
    "        my_not(assigned_z[node_fidx(d_idx=demand_idx, n_idx=V.index(v_of_demand))])\n",
    "        for demand_idx, demand in enumerate(demands)\n",
    "        for v_of_demand in list(demand.values())[1:]\n",
    "    )\n",
    "\n",
    "\n",
    "def constraint_flow_conservation(assigned_z):\n",
    "    total_flow = sum(\n",
    "        flow_conservation_per_node_per_demand(\n",
    "            node=node,\n",
    "            node_idx=node_idx,\n",
    "            demand=demand,\n",
    "            demand_idx=demand_idx,\n",
    "            assigned_z=assigned_z,\n",
    "        )\n",
    "        for node_idx, node in enumerate(V)\n",
    "        for demand_idx, demand in enumerate(demands)\n",
    "    )\n",
    "    return total_flow + constraint_starting_nodes(assigned_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_correlation(assigned_z):\n",
    "    return sum(\n",
    "        err_correlation[e1_idx, e2_idx]\n",
    "        * assigned_z[edge_fidx(0, e1_idx)]\n",
    "        * assigned_z[edge_fidx(1, e2_idx)]\n",
    "        for e1_idx in range(len(ordered_edges))\n",
    "        for e2_idx in range(len(ordered_edges))\n",
    "        if err_correlation[e1_idx, e2_idx] > 0\n",
    "    )\n",
    "\n",
    "\n",
    "def node_resiliency(assigned_z):\n",
    "    return sum(\n",
    "        B\n",
    "        * TotalCostNormalisation\n",
    "        * assigned_z[node_fidx(d_idx=0, n_idx=node_idx)]\n",
    "        * assigned_z[node_fidx(d_idx=1, n_idx=node_idx)]\n",
    "        for node_idx in range(len(V))\n",
    "        if V[node_idx] is not source_node\n",
    "    )\n",
    "\n",
    "\n",
    "def objective_minimal_resiliency(assigned_z):\n",
    "    return sum_correlation(assigned_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_hamiltonian(assigned_Z):\n",
    "    return (\n",
    "        objective_func(assigned_Z)\n",
    "        + node_resiliency(assigned_Z)\n",
    "        + objective_minimal_resiliency(assigned_Z)\n",
    "        + TotalConstraintNormalisation * (constraint_flow_conservation(assigned_Z))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Create QAOA Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LAYERS = 20\n",
    "\n",
    "\n",
    "@qfunc\n",
    "def mixer_layer(beta: CReal, qba: QArray[QBit]):\n",
    "    apply_to_all(lambda q: RX(GlobalScalingParameter * beta, q), qba)\n",
    "\n",
    "\n",
    "@qfunc\n",
    "def cost_layer(gamma: CReal, qba: QArray[QBit]):\n",
    "    phase(phase_expr=cost_hamiltonian(qba), theta=gamma)\n",
    "\n",
    "\n",
    "@qfunc\n",
    "def main(\n",
    "    params: CArray[CReal, 2 * NUM_LAYERS],\n",
    "    z: Output[QArray[QBit, D * row_len]],\n",
    ") -> None:\n",
    "    allocate(z)\n",
    "    hadamard_transform(z)\n",
    "\n",
    "    repeat(\n",
    "        count=NUM_LAYERS,\n",
    "        iteration=lambda i: (\n",
    "            cost_layer(params[2 * i], z),\n",
    "            mixer_layer(params[2 * i + 1], z),\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "qprog = synthesize(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(qprog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Width is {qprog.data.width}\")\n",
    "print(f\"Depth is {qprog.transpiled_circuit.depth}\")\n",
    "print(f\"Number of gates is {qprog.transpiled_circuit.count_ops}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe6a057-a8df-432e-9c72-4977134edec9",
   "metadata": {},
   "source": [
    "### Pre-execution setup\n",
    "\n",
    "We will create a `qaoa_samples()` function that will leverage the GPU accelerated Qiskit Aer simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b688157-fb12-4ba4-8634-ed867cac4893",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit import transpile\n",
    "from qiskit_aer import AerSimulator\n",
    "import qiskit.qasm3 as qasm3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e74250f-cd71-46f8-a800-80e5c64bd3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "qasm_string = str(qprog.transpiled_circuit.qasm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de13d121-47b4-4cec-b7ae-e65207e8028d",
   "metadata": {},
   "outputs": [],
   "source": [
    "qc = qasm3.loads(qasm_string)  \n",
    "qc.measure_all()\n",
    "\n",
    "# Here we will add the AMD GPU toggle\n",
    "sim = AerSimulator(device=\"GPU\")\n",
    "qc_t = transpile(qc, sim)\n",
    "\n",
    "params = qc_t.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e55514-35c0-40c4-9557-f55cc7560adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def make_aer_bind_dict(qc_t, values):\n",
    "    \"\"\"\n",
    "    Build Aer-compatible parameter_binds dict:\n",
    "      Parameter('params_param_k') -> [values[k]]\n",
    "    \"\"\"\n",
    "    values = list(values)  # works for np arrays too\n",
    "    bind = {}\n",
    "\n",
    "    seen = set()\n",
    "    for p in qc_t.parameters:\n",
    "        m = re.search(r\"params_param_(\\d+)$\", p.name)\n",
    "        if not m:\n",
    "            raise ValueError(f\"Unexpected parameter name: {p.name!r}\")\n",
    "        k = int(m.group(1))\n",
    "        if k in seen:\n",
    "            raise ValueError(f\"Duplicate parameter index {k} in circuit params\")\n",
    "        seen.add(k)\n",
    "\n",
    "        if k >= len(values):\n",
    "            raise ValueError(f\"Need values[{k}] but only {len(values)} values provided\")\n",
    "\n",
    "        bind[p] = [float(values[k])]   # <- IMPORTANT: list, not scalar\n",
    "\n",
    "    # Optional: sanity check that you covered exactly what you think you did\n",
    "    if len(seen) != len(values):\n",
    "        # Not always an error (circuit may use fewer params), but usually worth flagging\n",
    "        print(f\"Warning: circuit has {len(seen)} params, values has {len(values)}\")\n",
    "\n",
    "    return bind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4012fce-fff2-42cc-879a-17258c3d0999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qaoa_samples(samples,shots=1000, flip_endian: bool = True):\n",
    "\n",
    "    bind_dict = make_aer_bind_dict(qc_t, samples)\n",
    "\n",
    "    job = sim.run(qc_t, shots=shots, memory=True, parameter_binds=[bind_dict])\n",
    "    res = job.result()\n",
    "\n",
    "    samples = res.get_memory(0)\n",
    "\n",
    "    if flip_endian:\n",
    "        samples = [s[::-1] for s in samples]\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b7f80b-cad3-44a3-badb-14a154b1bf01",
   "metadata": {},
   "source": [
    "We will use a small converter to changet the qiskit output format back to Classiq format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e1335f-3845-4932-aa10-57180f41fe73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "\n",
    "class ParsedShot:\n",
    "    \"\"\"\n",
    "    Mimics Classiq's ParsedShot structure for compatibility.\n",
    "    \"\"\"\n",
    "    def __init__(self, state: Dict[str, List[int]], shots: int):\n",
    "        self.state = state\n",
    "        self.shots = shots\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"ParsedShot(state={self.state}, shots={self.shots})\"\n",
    "\n",
    "\n",
    "class ClassiqSampleResult:\n",
    "    \"\"\"\n",
    "    Mimics Classiq's sample result structure for compatibility.\n",
    "    \"\"\"\n",
    "    def __init__(self, parsed_counts: List[ParsedShot]):\n",
    "        self.parsed_counts = parsed_counts\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"ClassiqSampleResult(parsed_counts={self.parsed_counts})\"\n",
    "\n",
    "def qiskit_to_classiq_samples(\n",
    "    qiskit_samples: np.ndarray,\n",
    "    variable_name: str = \"z\",\n",
    "    wire_labels: Optional[Dict[str, int]] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Convert Qiskit sample output to Classiq ExecutionSession sample format.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert numpy array to list of lists for easier processing\n",
    "    if isinstance(qiskit_samples, np.ndarray):\n",
    "        if qiskit_samples.ndim == 1:\n",
    "            # Single shot, reshape to (1, n_qubits)\n",
    "            qiskit_samples = qiskit_samples.reshape(1, -1)\n",
    "        samples_list = qiskit_samples.tolist()\n",
    "    else:\n",
    "        samples_list = qiskit_samples\n",
    "    \n",
    "    # If wire_labels provided, reorder qubits according to labels\n",
    "    if wire_labels is not None:\n",
    "        # Create mapping from original index to new index\n",
    "        ordered_indices = [wire_labels[wire] for wire in sorted(wire_labels.keys())]\n",
    "        reordered_samples = []\n",
    "        for sample in samples_list:\n",
    "            reordered_sample = [sample[i] for i in ordered_indices]\n",
    "            reordered_samples.append(reordered_sample)\n",
    "        samples_list = reordered_samples\n",
    "    \n",
    "    # Count occurrences of each unique outcome\n",
    "    # Convert each sample to tuple for hashing, then count\n",
    "    sample_tuples = [tuple(sample) for sample in samples_list]\n",
    "    counts = Counter(sample_tuples)\n",
    "    \n",
    "    # Convert to ParsedShot objects\n",
    "    parsed_counts = []\n",
    "    for outcome_tuple, shot_count in counts.items():\n",
    "        outcome_list = list(outcome_tuple)\n",
    "        state = {variable_name: outcome_list}\n",
    "        parsed_shot = ParsedShot(state=state, shots=shot_count)\n",
    "        parsed_counts.append(parsed_shot)\n",
    "    \n",
    "    return ClassiqSampleResult(parsed_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Execute QAOA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SHOTS = 20000\n",
    "backend_preferences = ClassiqBackendPreferences(\n",
    "    backend_name=ClassiqSimulatorBackendNames.SIMULATOR\n",
    ")\n",
    "\n",
    "def initial_qaoa_params(NUM_LAYERS) -> np.ndarray:\n",
    "    initial_gammas = math.pi * np.linspace(\n",
    "        1 / (2 * NUM_LAYERS), 1 - 1 / (2 * NUM_LAYERS), NUM_LAYERS\n",
    "    )\n",
    "    initial_betas = math.pi * np.linspace(\n",
    "        1 - 1 / (2 * NUM_LAYERS), 1 / (2 * NUM_LAYERS), NUM_LAYERS\n",
    "    )\n",
    "\n",
    "    initial_params = []\n",
    "\n",
    "    for i in range(NUM_LAYERS):\n",
    "        initial_params.append(initial_gammas[i])\n",
    "        initial_params.append(initial_betas[i])\n",
    "\n",
    "    return np.array(initial_params)\n",
    "\n",
    "\n",
    "initial_params = initial_qaoa_params(NUM_LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98967299-c350-4bed-938d-e8cbb086b664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_cost(params):\n",
    "    samples = qaoa_samples(params)\n",
    "\n",
    "    samples = [[int(c) for c in row] for row in samples]\n",
    "\n",
    "    shot_costs = np.array([cost_hamiltonian(s) for s in samples], dtype=float)\n",
    "    objective_val = shot_costs.mean()\n",
    "    print(f\"Cost Hamiltonian = {np.round(objective_val, decimals=3)}\")\n",
    "\n",
    "    return objective_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_func = lambda state: cost_hamiltonian(state[\"z\"])\n",
    "\n",
    "\n",
    "def estimate_cost_func(params):\n",
    "    objective_val = estimate_cost(params.tolist())\n",
    "    print(f\"Cost Hamiltonian = {np.round(objective_val, decimals=3)}\")\n",
    "    return objective_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ITERATIONS = 60  # Increase to improve the results\n",
    "optimization_res = minimize(\n",
    "    estimate_cost_func,\n",
    "    x0=initial_params,\n",
    "    method=\"COBYLA\",\n",
    "    options={\"maxiter\": MAX_ITERATIONS},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = qaoa_samples(optimization_res.x.tolist(),shots=40000) #es.sample({\"params\": optimization_res.x.tolist()})\n",
    "res = qiskit_to_classiq_samples([[int(c) for c in row] for row in samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_validity(assigned_z: list[int]) -> bool:\n",
    "    if constraint_flow_conservation(assigned_z) != 0:\n",
    "        return False\n",
    "    if node_resiliency(assigned_z) != 0:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_counts = sorted(res.parsed_counts, key=lambda pc: pc.shots, reverse=True)\n",
    "\n",
    "\n",
    "def print_res(sampled, idx):\n",
    "    assigned_z = sampled.state[\"z\"]\n",
    "    if not check_validity(assigned_z):\n",
    "        return\n",
    "    valid_solutions_indices.append(idx)\n",
    "    print(\n",
    "        f\"Valid solution found at index {idx}, probability={sampled.shots/NUM_SHOTS*100}%, cost={cost_hamiltonian(assigned_z)}, latency={round(objective_func(assigned_z) / TotalCostNormalisation * (max_lat_guess - min_lat_guess))}\"\n",
    "    )\n",
    "\n",
    "\n",
    "valid_solutions_indices = []\n",
    "for idx, sampled in enumerate(sorted_counts[:100]):\n",
    "    print_res(sampled, idx)\n",
    "\n",
    "print(\n",
    "    f\"Accumulated valid probability is {sum(sampled.shots for sampled in sorted_counts if check_validity(sampled.state['z']))/NUM_SHOTS*100}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_1d_to_2d(array_1d):\n",
    "    array_2d = np.zeros((D, row_len))\n",
    "    for d_idx in range(D):\n",
    "        for e_idx in range(m):\n",
    "            array_2d[d_idx][e_idx] = array_1d[edge_fidx(d_idx, e_idx)]\n",
    "        for n_idx in range(n):\n",
    "            array_2d[d_idx][m + n_idx] = array_1d[node_fidx(d_idx, n_idx)]\n",
    "    return array_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_solution(index):\n",
    "    print(index)\n",
    "    solution = sorted_counts[index].state[\"z\"]\n",
    "    sol_2d = convert_1d_to_2d(solution)\n",
    "    print_solution_graph(sol_2d)\n",
    "    sol_df = pd.DataFrame(\n",
    "        sol_2d, index=[f\"{d['name']}({d['s']}→{d['t']})\" for d in demands], columns=ids\n",
    "    )\n",
    "    print(sol_df)\n",
    "    print(\n",
    "        f\"{objective_func(solution)/TotalCostNormalisation * (max_lat_guess-min_lat_guess)=}\"\n",
    "    )\n",
    "    print(f\"cost={cost_hamiltonian(solution)}\")\n",
    "    print(f\"Flow Conservation: {constraint_flow_conservation(solution)}\")\n",
    "    print(f\"Resiliency: {node_resiliency(solution)}\")\n",
    "\n",
    "\n",
    "# for index in valid_solutions_indices:\n",
    "for index in valid_solutions_indices:\n",
    "    print_solution(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a8c168-20c7-41d0-b503-50188439c1a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 9
}
